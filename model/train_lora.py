import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model

# ----------------------
# 1. åŸºæœ¬å‚æ•°
# ----------------------
MODEL_NAME = "Qwen/Qwen2.5-0.5B"
DATA_PATH = "data/gb_qa.jsonl"
OUTPUT_DIR = "lora-qlora-greenbuilding"
MAX_LENGTH = 512

# ----------------------
# 2. æ•°æ®åŠ è½½
# ----------------------
print("æ­£åœ¨åŠ è½½æ•°æ®â€¦â€¦")
dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# ----------------------
# 3. åŠ è½½ tokenizer å’Œæ¨¡å‹
# ----------------------
print("æ­£åœ¨åŠ è½½æ¨¡å‹â€¦â€¦")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

# ----------------------
# 4. LoRA é…ç½®
# ----------------------
lora_config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
print("LoRA å‚æ•°æ·»åŠ å®Œæ¯•")

# ----------------------
# 5. æ•°æ®å¤„ç†å‡½æ•°
# ----------------------
def format_example(example):
    instruction = example["instruction"]
    output = example["output"]
    text = f"ç”¨æˆ·æé—®ï¼š{instruction}\nå›ç­”ï¼š{output}"
    return tokenizer(
        text,
        max_length=MAX_LENGTH,
        truncation=True,
        padding="max_length"
    )

train_dataset = dataset.map(format_example)

# ----------------------
# 6. è®­ç»ƒå‚æ•°
# ----------------------
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=False,
    bf16=True,
    logging_steps=10,
    save_steps=200,
    save_total_limit=2,
    warmup_ratio=0.05,
    report_to="none"
)

data_collator = DataCollatorForSeq2Seq(tokenizer, padding=True)

# ----------------------
# 7. Trainer å¼€å§‹è®­ç»ƒ
# ----------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator
)

trainer.train()

# ----------------------
# 8. ä¿å­˜ LoRA æ¨¡å‹
# ----------------------
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("\nğŸ‰ LoRA å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š", OUTPUT_DIR)
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model

# ----------------------
# 1. åŸºæœ¬å‚æ•°
# ----------------------
MODEL_NAME = "Qwen/Qwen2.5-0.5B"
DATA_PATH = "data/gb_qa.jsonl"
OUTPUT_DIR = "lora-qlora-greenbuilding"
MAX_LENGTH = 512

# ----------------------
# 2. æ•°æ®åŠ è½½
# ----------------------
print("æ­£åœ¨åŠ è½½æ•°æ®â€¦â€¦")
dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# ----------------------
# 3. åŠ è½½ tokenizer å’Œæ¨¡å‹
# ----------------------
print("æ­£åœ¨åŠ è½½æ¨¡å‹â€¦â€¦")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

# ----------------------
# 4. LoRA é…ç½®
# ----------------------
lora_config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
print("LoRA å‚æ•°æ·»åŠ å®Œæ¯•")

# ----------------------
# 5. æ•°æ®å¤„ç†å‡½æ•°
# ----------------------
def format_example(example):
    instruction = example["instruction"]
    output = example["output"]
    text = f"ç”¨æˆ·æé—®ï¼š{instruction}\nå›ç­”ï¼š{output}"
    return tokenizer(
        text,
        max_length=MAX_LENGTH,
        truncation=True,
        padding="max_length"
    )

train_dataset = dataset.map(format_example)

# ----------------------
# 6. è®­ç»ƒå‚æ•°
# ----------------------
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=False,
    bf16=True,
    logging_steps=10,
    save_steps=200,
    save_total_limit=2,
    warmup_ratio=0.05,
    report_to="none"
)

data_collator = DataCollatorForSeq2Seq(tokenizer, padding=True)

# ----------------------
# 7. Trainer å¼€å§‹è®­ç»ƒ
# ----------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator
)

trainer.train()

# ----------------------
# 8. ä¿å­˜ LoRA æ¨¡å‹
# ----------------------
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("\nğŸ‰ LoRA å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š", OUTPUT_DIR)
